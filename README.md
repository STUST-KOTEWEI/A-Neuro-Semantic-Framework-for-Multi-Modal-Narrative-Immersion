# A Neuro-Semantic Framework for Multi-Modal Narrative Immersion 

## 專案願景 
故事的核心在於體驗，而非僅是文字。數百年來，我們透過視覺解碼符號來理解故事，但文字本身僅是通往故事世界的媒介。

Project H.O.L.O. 的使命，就是打破這個媒介的限制，提出一個大膽的問題：如果我們不僅能"閱讀"故事，而是能真正地"感受"它呢？ 

## 專案目標 
- 重新定義"閱讀"的體驗，讓讀者不僅僅是解讀文字，而是全方位感受故事中的情感與情境，成為故事的一部分。 

## 系統架構

本專案整合了四個核心子系統，共同實現多模態敘事體驗：

### 1️⃣ Image Recognition Subsystem (圖像識別子系統)
- **目標**：辨識書籍封面圖像與文字
- **技術**：Google Vision API、MobileNet、多語言OCR
- **功能**：支援模糊影像、低光源、離線/雲端雙模式

### 2️⃣ AI Content Generation Subsystem (內容生成子系統)
- **目標**：生成播客腳本與書籍摘要
- **技術**：GPT-4、BLOOM、Open Library API、Prompt Engineering
- **功能**：多語言支援、風格控制、智能推薦

### 3️⃣ Multi-sensory Output Subsystem (多感官輸出子系統)
- **目標**：轉化內容為多感官體驗
- **技術**：TTS、Stable Diffusion、音樂生成、字幕合成
- **功能**：語音、配圖、配樂、字幕、觸覺回饋

### 4️⃣ UI & Control Subsystem (使用者介面控制子系統)
- **目標**：簡潔直覺的使用介面
- **技術**：WAI-ARIA標準、跨平台框架
- **功能**：無障礙設計、個人化設定、多設備支援

> 📖 詳細的子系統文檔請參閱 [SUBSYSTEMS.md](./SUBSYSTEMS.md)

## 核心技術 
1. **深度語意分析**：
   - 使用自然語言處理 (NLP) 技術，將文本解構為語意單元。
   - 分析情感、語調、角色關係與故事背景。

2. **生成式 AI**：
   - 基於語意單元創建動態的聽覺體驗（例如角色對話、環境音效）。
   - 使用文本到聲音 (Text-to-Sound) 與文本到氣味 (Text-to-Scent) 的生成技術，模擬多感官回饋。

3. **多模態感知系統**：
   - 整合聽覺、觸覺與嗅覺回饋，打造沉浸式的敘事體驗。
   - 開發 API 供硬體設備（如觸覺反饋裝置）使用。

## 預期成果 
- 一個沉浸式敘事框架，能夠將任何文本轉化為多感官體驗。
- 支援多語言，應用於教育、娛樂與療癒場景。 

## 快速開始

### 後端設置
```bash
cd web/backend
pip install -r requirements.txt
uvicorn main:app --reload
```

### 前端設置
```bash
cd web/frontend
npm install
npm run dev
```

### API 文檔
啟動後端後，訪問 `http://127.0.0.1:8000/docs` 查看完整的 API 文檔。

## 主要功能

✨ **書籍掃描** - 上傳書籍封面，自動識別文字和獲取書籍資訊  
🎙️ **播客生成** - 將書籍內容轉換為播客腳本  
📝 **智能摘要** - 生成多種風格的書籍摘要  
🎵 **多感官體驗** - 配樂、配圖、字幕自動生成  
♿ **無障礙支援** - WAI-ARIA標準、鍵盤導航、螢幕閱讀器  
⚙️ **個人化設定** - 自訂語言、語速、主題等偏好

## 版權與貢獻 
歡迎對此專案感興趣的開發者提供意見並提交 PR.

## 技術文獻參考

詳細的技術文獻引用請參閱 [SUBSYSTEMS.md](./SUBSYSTEMS.md) 中各子系統的參考文獻章節。